{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Physical_Activity_18__Over_20112012.csv')\n",
    "df2 = df.to_parquet('Physical_Activity.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management\n",
    "\n",
    "Data Management is hard, and before you know it, you can ended up with `final_final_final_project_data-2019.csv.bak` as the source of your project data.\n",
    "\n",
    "Below is a series of tips, tricks and usecases for managing data throughout the lifecycle of a projects. \n",
    "\n",
    "\n",
    "## Reading and Writing Data \n",
    "### S3\n",
    "Our team often uses Amazon S3 as a bucket storage. To access data in S3, you'll have to have AWS access credentials stored at `~/.aws/credentials` per the [documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). \n",
    "\n",
    "S3 can store anything, of arbitrary object size and shape. It's like a giant folder in the cloud. You can use it to store CSVs, Pickes, Videos, whatever. \n",
    "\n",
    "To write and read a pandas dataframe as a CSV to S3: \n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "df.to_csv(s3://bucket-name/my_df_name.csv)\n",
    "\n",
    "pd.read_csv(s3://bucket-name/my_df_name.csv)\n",
    "```\n",
    "\n",
    "You can read more in the [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) docs. By putting data on S3, anybody on the team can use / access/ and replicate without having to transfer data files between machines. \n",
    "\n",
    "### Local Folders \n",
    "Sometimes, it is easiest to simply use your local file system to store data. As a manner of conventions, we use the `data` and `processed` directories, which  are automatically ignored from git from the project base `.gitignore`. \n",
    "\n",
    "`/data` should be used for raw data, while `/processed` should be used for any intermediate steps. You can also duplicate this on S3. Finally, `outputs/` for outputs. This should all best setup by our [data science template](https://github.com/CityOfLosAngeles/cookiecutter-data-science). If you use `data`, make sure all the data in it is documented in your README. \n",
    "\n",
    "Don't hardcode paths, such as `//Users/YOUR_EID/. You might need to use S3 or a cloud database if your data is big. \n",
    "\n",
    "### Databases \n",
    "Finally, for analysis and storage purposes, it is sometimes best to store the data in a structured database for querying. We use `postgreSQL` for this purposes. \n",
    "\n",
    "To access, you'll need to set the $POSTGRES_URI [environment variable](https://devblogs.microsoft.com/commandline/share-environment-vars-between-wsl-and-windows/) equal to the proper connection string. Once that is set, you'll be able to read and write SQL queries into dataframes as follows: \n",
    "\n",
    "```\n",
    "import os \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd \n",
    "\n",
    "connection_string = os.environ.get('POSTGRES_URI')\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "df.to_sql(\n",
    "    \"my_table_name\",\n",
    "    engine,\n",
    "    schema=\"public\",\n",
    "    if_exists=\"replace\",\n",
    "    dtype={\"geom\": Geometry(\"POINT\", srid=srid)},\n",
    ")\n",
    "\n",
    "query = \"SELECT * FROM my_table_name LIMIT 10;\"\n",
    "\n",
    "pd.read_sql(query,engine) \n",
    "\n",
    "```\n",
    "\n",
    "Note: This example shows how to make sure that Geometry types are inserted into POSTGIS (a point example) and limits the number of rows returned to 10 in the Query. You can execute abritrary SQL inside `pd.read_sql()`.\n",
    "\n",
    "## Formats and use-cases \n",
    "Data Interchange: Where everything can be broken.\n",
    "\n",
    "### CSVs \n",
    "CSVs are the lowest common denominator of data files. They are plain text files that contain a list of data. Best for getting raw data from SQL and storing large blobs on cloud services. For interchange, it is better to use Parquet or even Excel as they preserve datatypes.\n",
    "\n",
    "Benefits to CSVs include their readability and ease of use for users. Part of the reason they are so common is their ability to be easily viewed and understood by users. Unlike Parquet files, they are not compressed and condensed and are therefore a cleaner read.\n",
    "\n",
    "The downsides to CSVs are their inability to compress down. File sizes can easily get out of hand with CSVs, making Parquet files a preferable alternative in that regard. They also don't store data types for columns. If there are different data types within a single column, this can lead to numerous isses. For example, if there are both strings and integers mixed into a single column, the process of analyzing that CSV becomes extremely difficult and even impossible at times. Finally, another key issue with CSVs is the ability to only store a single sheet in a file without any formatting or formulas. Parquet and Feather files do a better job of allowing for formulas and different formats.\n",
    "\n",
    "\n",
    "### Excel / XLSX\n",
    "\n",
    "Best for sharing with other teams, except for geographic info (use shapefiles or geojson instead). Excel/XLSX is a binary file format that holds information about all the worksheets in a file, including both content and formatting. This means Excel files are capable of holding formatting, images, charts, forumlas, etc. CSVs are more limited in this regard. A downside to Excel files are that they aren't commonly readable by data upload interfaces. Every data upload interface is capable of processing CSVs, but Excel files often require extensions in order to be parsed. The ease of processing CSVs makes it easier to move data between different platforms than with Excel files.\n",
    "\n",
    "You often might want to write multiple dataframes to a single excel files as sheets. Here's a guide: \n",
    "\n",
    "```\n",
    "## init a writer \n",
    "writer = pd.ExcelWriter('../outputs/filename.xlsx', engine='xlsxwriter')\n",
    "\n",
    "## assume district_dfs is a list of dataframes by council district \n",
    "Write each dataframe to a different worksheet.\n",
    "for key, value in district_dfs.items(): \n",
    "    value.to_excel(writer, sheet_name=key)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "```\n",
    "### Parquet \n",
    "Parquet is an \"open source columnar storage format for use in data analysis systems\". Columnar storage is more efficient as it is more easily compressed and the data is more homogenous. CSV files are in the row-based storage format, a contributing factor for why Parquets files are more preferred especially as the size of dataset gets larger and larger. Parquet files are faster to read than CSVs, as it has a higher querying speed, preserve datatypes (ie, Number,  Timestamps, Points). They are best for intermediate data storage and large datasets (1GB+) on S3. Some of the downsides to Parquet files are their unreadability and their lack of built-in support for categorical data. Similarly, even though the file is smaller and more manageable, it requires an extra amount of resources to uncompress the data back into a dataframe. It requires more loading power to uncompress the data because the file is so compressed.\n",
    "\n",
    "Also good for passing dataframes between Python and R. A similar option is [feather](https://blog.rstudio.com/2016/03/29/feather/).\n",
    "\n",
    "Not good for being able to quickly look at the dataset in GUI based (Excel, QGIS, etc) programs. \n",
    "[Docs](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "Here is a way to use pandas to convert a local CSV file to a Parquet file:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Physical_Activity_18__Over_20112012.csv')\n",
    "df2 = df.to_parquet('Physical_Activity.parquet')\n",
    "```\n",
    "\n",
    "\n",
    "### Feather Files\n",
    "Feather provides a lightweight binary columnar serialization format for data frames. It is designed to make reading and writing data frames more efficient, as well as to make sharing data across data analysis languages easier. The goal is to push data frames in and out of memory with minimal work. Just like Parquet, Feather is also capable of passing dataframes between Python and R, as well as storing column data types. \n",
    "\n",
    "Benefits of using Feather involve its ability to not use internal compression, allowing it to work best with solid-state drives. Similarly, Feather doesn't need unpacking in order to load it back into RAM. \n",
    "\n",
    "Feather might not be the ideal file format if you're looking for long-term data storage. It is really only equipped for short-term data storage. The Feather files themselves are smaller than CSVs, but they don't necessarily have the same level of compression as Parquet files. It has a higher I/O speed, but the reduction in file size from a CSV isn't at the same level of Parquet.\n",
    "\n",
    "Once you install the feather package with `$ pip install feather-format` then you can easily write a dataframe.\n",
    "\n",
    "```\n",
    "import feather\n",
    "path = 'my_data.feather'\n",
    "feather.write_dataframe(df, path)\n",
    "df = feather.read_dataframe(path)\n",
    "```\n",
    "\n",
    "### GeoJSON:\n",
    "GeoJSON is an open-standard format for encoding a variety of geographic data structures usin JavaScript Object Notation (JSON). A GeoJSON object may represent a region of space (a Geometry), a spatially bounded entity (a Feature), or a list of Features (a FeatureCollection). It supports geometry types: Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection. JSON is light and easier to read, but GeoJSON files can quickly get too large to handle.\n",
    " \n",
    "\n",
    "### Shapefiles \n",
    "Shapefiles are a geospatial vector data format for geographic information system software and the original file format for geospatial data. They are capable of spatially describing vector features: points, lines, and polygons. Geopandas has good support for reading / writing shapefiles. \n",
    "\n",
    "One weird thing, however, is that a shapefile isn't a _file_, it's a _folder_, containing multiple subfiles (such as .dbf, .shpx, etc). To properly read/write shapefiles, make sure to read the entire folder or write to a folder each time. This can cause issues especially as most shapefiles are compressed into a zip file with isn't always easily decompressed. \n",
    "\n",
    "It is often better to use `geojson` vs `shapefiles` since the former is easier to render on the web. The latter is better when you have a bespoke projection. A few downsides to shapefiles include their inability to store topolgical information and the fize size restriction of 2GB. Similarly, shapefiles can only contain one geometry type per file. \n",
    "\n",
    "```\n",
    "import geopandas as gpd \n",
    "import os\n",
    "\n",
    "# read shapefile \n",
    "gpd.read_file('./my_folder/')\n",
    "\n",
    "# write shapefile \n",
    "if not os.path.exists('./outputs/my_dir_name'):\n",
    "    os.mkdirs('./outputs/my_dir_name')\n",
    "gdf.to_file('./outputs/my_dir_name')\n",
    "```\n",
    "\n",
    "### PBF (Protocolbuffer Binary Format):\n",
    "Protocol Buffers is a method of serialized structured dta. It is used for storing and interchanging structured information of all types. PRB involves an interface description language that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data. As compared to XML, it is designed to be simpler and quicker. A benefit of using PBF is that it is easy to bind to objects. A consequence of using PBF is that parsing is sequential in the Protobuf library.\n",
    "\n",
    "### Databases \n",
    "A whole field of study, it is often useful to use a DB for analytics and aggegrated queries, rather than just your production datastore. Our team maintains a Postgresql + PostGIS DB to help us make complex spatial + geospatial queries. However, it is best practice to move those queries to python or a `Makefile` ASAP. \n",
    "\n",
    "### Pickles\n",
    "A way of  storing arbitrary python objects. Danger lives here. \n",
    "\n",
    "### GeoJSON \n",
    "The other important open geodata spec, geojson is often easier to work with than a shapefile. However, it can get very large very quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
